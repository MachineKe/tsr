{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcf81337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- üì¶ Imports ---\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- üí° Utilities to Parse XML Annotations ---\n",
    "def parse_structure_annotations(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    boxes, labels = [], []\n",
    "    for obj in root.findall('object'):\n",
    "        label = obj.find('name').text\n",
    "        bbox = obj.find('bndbox')\n",
    "        box = [\n",
    "            int(float(bbox.find('xmin').text)),\n",
    "            int(float(bbox.find('ymin').text)),\n",
    "            int(float(bbox.find('xmax').text)),\n",
    "            int(float(bbox.find('ymax').text)),\n",
    "        ]\n",
    "        boxes.append(box)\n",
    "        labels.append(label)\n",
    "    return boxes, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf45da",
   "metadata": {},
   "source": [
    "Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8aba5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "class PubTablesTSRDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, label_map, transforms=None, max_samples=3500):\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.ann_dir = Path(ann_dir)\n",
    "        self.transforms = transforms\n",
    "        self.label_map = label_map\n",
    "\n",
    "        # ‚úÖ Keep only valid image-annotation pairs\n",
    "        valid_files = sorted([\n",
    "            f for f in self.img_dir.glob(\"*.jpg\")\n",
    "            if (self.ann_dir / (f.stem + \".xml\")).exists()\n",
    "        ])\n",
    "\n",
    "        # ‚úÖ Limit to first N valid samples\n",
    "        self.img_files = valid_files[:max_samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def random_valid_index(self, idx):\n",
    "        return (idx + 1) % len(self)\n",
    "\n",
    "    def __getitem__(self, idx, retry_count=0):\n",
    "        try:\n",
    "            img_path = self.img_files[idx]\n",
    "            ann_path = self.ann_dir / (img_path.stem + \".xml\")\n",
    "\n",
    "            if not img_path.exists() or not ann_path.exists():\n",
    "                raise FileNotFoundError(f\"Missing file: {img_path} or {ann_path}\")\n",
    "\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            boxes, labels = parse_structure_annotations(ann_path)\n",
    "\n",
    "            # Filter labels and boxes together\n",
    "            filtered = [(box, lbl) for box, lbl in zip(boxes, labels) if lbl in self.label_map]\n",
    "            if len(filtered) == 0:\n",
    "                raise ValueError(f\"No valid boxes/labels in {ann_path}\")\n",
    "\n",
    "            boxes, labels = zip(*filtered)\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor([self.label_map[lbl] for lbl in labels], dtype=torch.int64)\n",
    "\n",
    "            target = {\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": labels,\n",
    "                \"image_id\": torch.tensor([idx])\n",
    "            }\n",
    "\n",
    "            if self.transforms:\n",
    "                image = self.transforms(image)\n",
    "            else:\n",
    "                image = F.to_tensor(image)\n",
    "\n",
    "            return image, target\n",
    "\n",
    "        except (FileNotFoundError, ValueError, KeyError) as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping idx {idx}: {e}\")\n",
    "            if retry_count < 3:\n",
    "                return self.__getitem__(self.random_valid_index(idx), retry_count + 1)\n",
    "            else:\n",
    "                raise RuntimeError(\"Too many invalid samples in a row.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cfdcb",
   "metadata": {},
   "source": [
    "Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53196590",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    \"table\": 0,\n",
    "    \"table row\": 1,\n",
    "    \"table column\": 2,\n",
    "    \"column header\": 3,\n",
    "    \"projected row header\": 4,\n",
    "    \"spanning cell\": 5,\n",
    "    \"no cell\": 6,\n",
    "    \"table spanning cell\": 7,\n",
    "    \"table column header\": 8,\n",
    "    \"table projected column header\": 9,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b17610",
   "metadata": {},
   "source": [
    "Setup Transforms and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73b3e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- üîÅ Transformations ---\n",
    "transform = T.Compose([\n",
    "    T.Resize((512, 512)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "# --- üìÅ Paths ---\n",
    "base_path = Path(\"archive\")  # replace this with your dataset root\n",
    "train_dataset = PubTablesTSRDataset(base_path / \"images\", base_path / \"train\", label_map, transforms=transform)\n",
    "val_dataset = PubTablesTSRDataset(base_path / \"images\", base_path / \"val\", label_map, transforms=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11c3954",
   "metadata": {},
   "source": [
    "Model (Using Faster R-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95eccde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=10, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=40, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# --- ‚öôÔ∏è Load pre-trained Faster R-CNN ---\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, len(label_map))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bdd69",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39653221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for images, targets in tqdm(loader, desc=\"Training\"):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62f6ab",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321c3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    for images, targets in tqdm(loader, desc=\"Evaluating\"):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        outputs = model(images)\n",
    "        # you can add evaluation metrics here like IoU, AP etc\n",
    "    print(\"‚úÖ Evaluation completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9288d56",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f444fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 875/875 [6:40:44<00:00, 27.48s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìâ Training Loss: 1.5911\n",
      "üíæ Saved checkpoint: checkpoints/tsr_model_epoch_1.pth\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nüìò Epoch {epoch+1}/{epochs}\")\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"üìâ Training Loss: {loss:.4f}\")\n",
    "\n",
    "    # Save checkpoint every epoch\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "    checkpoint_path = f\"checkpoints/tsr_model_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"üíæ Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c759c7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1750/1750 [2:15:08<00:00,  4.63s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation completed.\n",
      "‚úÖ Saved best model (best_tsr_model.pth)\n",
      "‚úÖ Saved final model (final_tsr_model.pth)\n"
     ]
    }
   ],
   "source": [
    "# Validation loss tracking (you can refine this with actual val loss or mAP)\n",
    "evaluate_model(model, val_loader, device)\n",
    "\n",
    "    # Save best model\n",
    "if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        torch.save(model.state_dict(), \"best_tsr_model.pth\")\n",
    "        print(\"‚úÖ Saved best model (best_tsr_model.pth)\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"final_tsr_model.pth\")\n",
    "print(\"‚úÖ Saved final model (final_tsr_model.pth)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
